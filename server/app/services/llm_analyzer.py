"""
LLM Analyzer Service
Uses Gemini or GPT APIs to analyze transcribed content for AI generation and dangerous content
"""

from typing import Dict, Any, Optional
from app.core.config import settings


class LLMAnalyzer:
    """Service for analyzing content using LLM APIs (Gemini or OpenAI GPT)"""
    
    def __init__(self, provider: str = None):
        """
        Initialize LLM analyzer
        
        Args:
            provider: LLM provider ('gemini' or 'openai')
                     Defaults to settings.LLM_PROVIDER
        """
        self.provider = provider or settings.LLM_PROVIDER
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize the appropriate LLM client"""
        if self.provider == "gemini":
            import google.generativeai as genai
            genai.configure(api_key=settings.GEMINI_API_KEY)
            self.client = genai.GenerativeModel('gemini-2.5-flash')
            print("‚úÖ Gemini client initialized (gemini-2.5-flash)")
        
        elif self.provider == "openai":
            from openai import OpenAI
            self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
            print("‚úÖ OpenAI client initialized")
        
        else:
            raise ValueError(f"Unsupported LLM provider: {self.provider}")
    
    def analyze_content(self, transcription: str, video_metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Analyze transcribed content for AI generation and dangerous content
        
        Args:
            transcription: Full text transcription from Whisper
            video_metadata: Optional video metadata (title, description, etc.)
        
        Returns:
            Dictionary with analysis results:
            - ai_generated_score: Probability (0-100) that content is AI-generated
            - ai_indicators: List of specific indicators of AI generation
            - dangerous_content: Boolean if dangerous content detected
            - danger_categories: List of danger categories found
            - danger_severity: Severity level (low, medium, high, critical)
            - explanation: Detailed explanation of the analysis
            - confidence: Confidence level of the analysis (0-100)
        """
        prompt = self._build_analysis_prompt(transcription, video_metadata)
        
        print(f"ü§ñ Analyzing content with {self.provider.upper()}...")
        
        if self.provider == "gemini":
            result = self._analyze_with_gemini(prompt)
        else:  # openai
            result = self._analyze_with_openai(prompt)
        
        # Parse the LLM response
        analysis = self._parse_llm_response(result)
        
        print(f"‚úÖ Analysis complete: AI Score={analysis['ai_generated_score']}%, "
              f"Dangerous={analysis['dangerous_content']}")
        
        return analysis
    
    def _build_analysis_prompt(self, transcription: str, metadata: Optional[Dict] = None) -> str:
        """Build the analysis prompt for the LLM"""
        
        metadata_text = ""
        if metadata:
            metadata_text = f"""
Video Metadata:
- Title: {metadata.get('title', 'Unknown')}
- Uploader: {metadata.get('uploader', 'Unknown')}
- Description: {metadata.get('description', 'N/A')}
"""
        
        prompt = f"""You are an expert content analyzer. Analyze the following YouTube video transcription for two key aspects:

1. **AI-Generated Content Detection**: Determine if the content appears to be AI-generated (e.g., generated by ChatGPT, AI voice synthesis, automated script generation)
2. **Dangerous Content Detection**: Identify any potentially harmful, dangerous, illegal, or policy-violating content

{metadata_text}

Transcription:
{transcription[:4000]}

Please provide your analysis in the following JSON format:
{{
    "ai_generated_score": <0-100>,
    "ai_indicators": ["indicator1", "indicator2", ...],
    "dangerous_content": <true/false>,
    "danger_categories": ["category1", "category2", ...],
    "danger_severity": "<low/medium/high/critical>",
    "explanation": "<detailed explanation>",
    "confidence": <0-100>
}}

AI Indicators may include: repetitive patterns, unnatural phrasing, lack of emotion, generic content, robotic narration, etc.
Danger Categories may include: violence, hate speech, misinformation, illegal activities, self-harm, adult content, scams, etc.

Respond ONLY with the JSON, no additional text."""
        
        return prompt
    
    def _analyze_with_gemini(self, prompt: str) -> str:
        """Analyze using Google Gemini"""
        response = self.client.generate_content(prompt)
        return response.text
    
    def _analyze_with_openai(self, prompt: str) -> str:
        """Analyze using OpenAI GPT"""
        response = self.client.chat.completions.create(
            model="gpt-4-turbo-preview",  # or "gpt-3.5-turbo" for faster/cheaper
            messages=[
                {"role": "system", "content": "You are an expert content analyzer specializing in AI-generated content detection and dangerous content identification."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # Lower temperature for more consistent analysis
        )
        return response.choices[0].message.content
    
    def _parse_llm_response(self, response_text: str) -> Dict[str, Any]:
        """Parse LLM JSON response with error handling"""
        import json
        import re
        
        try:
            # Extract JSON from response (in case there's extra text)
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_str = json_match.group(0)
                result = json.loads(json_str)
            else:
                raise ValueError("No JSON found in response")
            
            # Validate and set defaults
            return {
                "ai_generated_score": int(result.get("ai_generated_score", 0)),
                "ai_indicators": result.get("ai_indicators", []),
                "dangerous_content": bool(result.get("dangerous_content", False)),
                "danger_categories": result.get("danger_categories", []),
                "danger_severity": result.get("danger_severity", "low"),
                "explanation": result.get("explanation", ""),
                "confidence": int(result.get("confidence", 50)),
            }
        
        except (json.JSONDecodeError, ValueError) as e:
            print(f"‚ö†Ô∏è Error parsing LLM response: {e}")
            # Return safe defaults if parsing fails
            return {
                "ai_generated_score": 0,
                "ai_indicators": [],
                "dangerous_content": False,
                "danger_categories": [],
                "danger_severity": "low",
                "explanation": f"Analysis failed: {str(e)}",
                "confidence": 0,
            }
